{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "hRTa3Ee15WsJ"
   },
   "source": [
    "# Exercise W11:  Image Classification with Transfer learning and fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2X4KyhORdSeO"
   },
   "source": [
    "In this exercise we will dig into the topic of image classification, using pretrained networks, both using transfer learning and then also fine-tuning the network to our use case.\n",
    "\n",
    "First we will get a feeling for convolution and data augmentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "TqOt6Sv7AsMi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import PIL.Image as Image\n",
    "from IPython.display import Image, display\n",
    "import matplotlib.cm as cm\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3P99QiMGit1A",
    "tags": []
   },
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'): #this line is essential only if you are using a M1 Processor\n",
    "    data_augmentation = tf.keras.Sequential([\n",
    "      ### HERE INSERT ONE OF MORE LAYERS TO PERFORM DATA AUGMENTATION#####  \n",
    "      # see https://www.tensorflow.org/api_docs/python/tf/keras/layers/     \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img =  tf.keras.utils.load_img(\"your_chosen_image.jpg\")\n",
    "img = tf.cast(tf.expand_dims(img, 0), tf.float32)\n",
    "\n",
    "#if the two lines above give an error related with casting, PiL and Tensor, chnage to the follwoing\n",
    "#img =  tf.keras.utils.load_img(\"your_chosen_image.jpg\")\n",
    "#img =  tf.keras.utils.img_to_array(img)\n",
    "#img =  tf.expand_dims(img, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    for i in range(9):\n",
    "      augmented_image = data_augmentation(img)\n",
    "      ax = plt.subplot(3, 3, i + 1)\n",
    "      plt.imshow(augmented_image[0] / 255)\n",
    "      plt.axis(\"off\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Face Classification with Transfer Learning & Fine Tuning: Smile vs Not Smile!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0GoKGm1duzgk"
   },
   "source": [
    "### Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHP9qMJxt2oz"
   },
   "source": [
    "We will use the Genki4k Database, which contains 2162 Imgs of smiling people and 1838 of not smiling people. You find the dataset in Moodle, already organised in 2 subfolders (smiling, not_smiling). This is particularly useful because the utility we will use to load images ('tf.keras.utils.image_dataset_from_directory') infers labels from the subdirectory. More info also here: (https://www.tensorflow.org/tutorials/load_data/images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ro4oYaEmxe4r",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.curdir, 'genki4k/')\n",
    "                        \n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (192, 192)\n",
    "\n",
    "train_dataset = tf.keras.utils.image_dataset_from_directory(data_dir,\n",
    "                                                            batch_size=BATCH_SIZE,\n",
    "                                                            image_size=IMG_SIZE,\n",
    "                                                            shuffle=True,\n",
    "                                                            seed=999,\n",
    "                                                            validation_split=0.20,\n",
    "                                                            subset = \"training\")\n",
    "\n",
    "validation_dataset = tf.keras.utils.image_dataset_from_directory(data_dir,\n",
    "                                                            batch_size=BATCH_SIZE,\n",
    "                                                            image_size=IMG_SIZE,\n",
    "                                                            shuffle=True,\n",
    "                                                            seed=999,\n",
    "                                                            validation_split=0.20,\n",
    "                                                            subset = \"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K5BeQyKThC_Y",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_names = train_dataset.class_names\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_dataset.take(1):\n",
    "  for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "    plt.title(class_names[labels[i]])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZqCX_mpV3Mx"
   },
   "source": [
    "We need also to create a test set, that it what the next cell does (basically split the validation in batches and keep 1 for the test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uFFIYrTFV9RO",
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_batches = tf.data.experimental.cardinality(validation_dataset)\n",
    "test_dataset = validation_dataset.take(val_batches // 5)\n",
    "validation_dataset = validation_dataset.skip(val_batches // 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q9pFlFWgBKgH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Number of validation batches: %d' % tf.data.experimental.cardinality(validation_dataset))\n",
    "print('Number of test batches: %d' % tf.data.experimental.cardinality(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22XWC7yjkZu4"
   },
   "source": [
    "It is convenient to use buffered prefetching to load images from disk without having I/O become blocking. That is what the next cell does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p3UUPdm86LNC",
    "tags": []
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYfcVwYLiR98"
   },
   "source": [
    "### Use data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDWc5Oad1daX"
   },
   "source": [
    "As we saw in the class to reduce overfitting, we can increase the diversity within the data by performing data augmentation. Insert here some of the trasnformation you experimented before. And later do notice how actually this doesn't increaese the sample size!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3P99QiMGit1A",
    "tags": []
   },
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    data_augmentation = tf.keras.Sequential([\n",
    "      ###### INSERT HERE KERAS LAYERS TO PERFORM DATA AUGMENTAION######\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9SlcbhrarOO"
   },
   "source": [
    "Note: Data augmentation is only active during training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9mD3rE2Lm7-d"
   },
   "source": [
    "Let's repeatedly apply these layers to the same image and see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aQullOUHkm67",
    "tags": []
   },
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    for image, _ in train_dataset.take(1):\n",
    "      plt.figure(figsize=(10, 10))\n",
    "      first_image = image[0]\n",
    "      for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        augmented_image = data_augmentation(tf.expand_dims(first_image, 0))\n",
    "        plt.imshow(augmented_image[0] / 255)\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAywKtuVn8uK"
   },
   "source": [
    "### Choose Transfer Learning Model \n",
    "\n",
    "As a first execution of the notebook, we will use the following pre-trained model: **MobileNetV2** developed by Google (dowloaded via  `tf.keras.applications.MobileNetV2` ). This will be your base model. The big advantage is that it was trained on ImageNet data! So 1.4M images :) with a wide variety of categories like `jackfruit` and `syringe`. Not on smiles though...Will this help us in recognizing smiles??\n",
    "\n",
    "Note: Every model was trained using specific image preprocessing requirements. YOU NEED TO CHECK EACH TIME WHAT PECULIAR OF A SPECIFIC MODEL!\n",
    "Here the guide for MobileNetV2 https://keras.io/api/applications/mobilenet/.\n",
    "\n",
    "This model expects pixel values in `[-1, 1]`, but at this point, the pixel values in your images are in `[0, 255]`. \n",
    "At best, you always use the preprocessing method included with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cO0HM9JAQUFq",
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "OkH-kazQecHB"
   },
   "source": [
    "We saw in the lesson that, when doing transfer learning, we don't use all the architecture but only the feature extraction.\n",
    "The very last classification layer (on \"top\", as most diagrams of machine learning models go from bottom to top) is not very useful. Instead, you will follow the common practice to depend on the very last layer before the flatten operation. This layer is called the \"bottleneck layer\". The bottleneck layer features retain more generality as compared to the final/top layer.\n",
    "\n",
    "First, instantiate a MobileNet V2 model pre-loaded with weights trained on ImageNet. By specifying the **include_top=False** argument, you load a network that doesn't include the classification layers at the top, which is ideal for feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "19IQ2gqneqmS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the base model from the pre-trained model MobileNet V2\n",
    "\n",
    "IMG_SHAPE = IMG_SIZE + (3,)\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqcsxoJIEVXZ"
   },
   "source": [
    "This feature extractor converts each `192x192x3` image into a `6x6x1280` block of features. Let's see what it does to an example batch of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y-2LJL0EEUcx",
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_batch, label_batch = next(iter(train_dataset))\n",
    "feature_batch = base_model(image_batch)\n",
    "print(feature_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rlx56nQtfe8Y"
   },
   "source": [
    "### a. Freeze weights, use the feature extraction as it is, attach a new classification head\n",
    "In this step, you will freeze the convolutional base created from the previous step and to use as a feature extractor. Additionally, you add a classifier on top of it and train the top-level classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fL6upiN3ekS"
   },
   "source": [
    "It is important to freeze the convolutional base before you compile and train the model. Freezing (by setting layer.trainable = False) prevents the weights in a given layer from being updated during training. MobileNet V2 has many layers, so setting the entire model's `trainable` flag to False will freeze all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OTCJH4bphOeo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the base model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KpbzSmPkDa-N",
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now combine this model with a classification head, meaning some fully connected layers (tf.keras.layers.Dense). \n",
    "\n",
    "Build a model by chaining together the data augmentation, preproccesing, `base_model` and classification layers (note that the syntax is slighly different, we are using the [Keras Functional API]) https://www.tensorflow.org/guide/keras/functional .\n",
    "\n",
    "In the Functional API every layer is a \"function\" which is applied to the output of the previous layer, s. Slide 23 for example. \n",
    "The architecture we want to implement is structured as follows:\n",
    "\n",
    "-Input layer\n",
    "\n",
    "-data augmentation layer (we defined above) \n",
    "\n",
    "-preprocessing layer (we defined above)\n",
    "\n",
    "-pretrained base model !!!!Here you need to set the parameter \"training=False\" because the model contains a `BatchNormalization` layer. More explanation here https://www.tensorflow.org/guide/keras/transfer_learning\n",
    "\n",
    "-Gobal Average Pooling2d layer. This layer has the same purpose as tf.keras.layers.Flatten(). Need to connect something 2D to something 1D.\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling2D  \n",
    "\n",
    "-dropout layer\n",
    "\n",
    "-output layer \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "O1p0OJBR6dOT"
   },
   "source": [
    "Apply a `tf.keras.layers.Dense` layer to convert these features into a single prediction per image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DgzQX6Veb2WT",
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=IMG_SHAPE)\n",
    "x = #data augmentation layer\n",
    "x = #preprocessing layer\n",
    "x = #base_model -> remember to set \"training=False\"\n",
    "x = #GlobalAveragePooling2D \n",
    "x = #Dropout\n",
    "outputs = #output layer, aka a Dense Layer with rigth number of neurons and right activation function \n",
    "model = tf.keras.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0ylJXE_kRLi"
   },
   "source": [
    "Compile the model before training it. Since there are two classes, use the `tf.keras.losses.BinaryCrossentropy` loss with `from_logits=True` since the model provides a linear output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RpR8HdyMhukJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_learning_rate = 0.0001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I8ARiyMFsgbH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lxOcmVr0ydFZ"
   },
   "source": [
    "Above you see how many parameters you are actually training, and how many are frozen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RxvgOYTDSWTx"
   },
   "source": [
    "### Train the model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Om4O3EESkab1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial_epochs = 20\n",
    "\n",
    "loss0, accuracy0 = model.evaluate(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8cYT1c48CuSd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"initial loss: {:.2f}\".format(loss0))\n",
    "print(\"initial accuracy: {:.2f}\".format(accuracy0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JsaRFlZ9B6WK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset,\n",
    "                    epochs=initial_epochs,\n",
    "                    validation_data=validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hd94CKImf8vi"
   },
   "source": [
    "\n",
    "Let's take a look at the learning curves of the training and validation accuracy/loss when using the MobileNetV2 base model as a fixed feature extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "53OTCh3jnbwV",
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "foWMyyUHbc1j"
   },
   "source": [
    "Note: If you are wondering why the validation metrics are better than the training metrics, the main factor is because layers like `tf.keras.layers.BatchNormalization` and `tf.keras.layers.Dropout` affect accuracy during training. They are turned off when calculating validation loss.\n",
    "\n",
    "To a lesser extent, it is also because training metrics report the average for an epoch, while validation metrics are evaluated after the epoch, so validation metrics see a model that has trained slightly longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CqwV-CRdS6Nv"
   },
   "source": [
    "### b) Fine tuning\n",
    "In the feature extraction experiment, you were only training a few layers on top of an MobileNetV2 base model. The weights of the pre-trained network were **not** updated during training.\n",
    "\n",
    "One way to increase performance even further is to train (or \"fine-tune\") the weights of the top layers of the pre-trained model alongside the training of the classifier you added. The training process will force the weights to be tuned from generic feature maps to features associated specifically with the dataset.\n",
    "\n",
    "Note: This should only be attempted after you have trained the top-level classifier with the pre-trained model set to non-trainable. If you add a randomly initialized classifier on top of a pre-trained model and attempt to train all layers jointly, the magnitude of the gradient updates will be too large (due to the random weights from the classifier) and your pre-trained model will forget what it has learned.\n",
    "\n",
    "Also, you should try to fine-tune a small number of top layers rather than the whole MobileNet model. In most convolutional networks, the higher up a layer is, the more specialized it is. The first few layers learn very simple and generic features that generalize to almost all types of images. As you go higher up, the features are increasingly more specific to the dataset on which the model was trained. The goal of fine-tuning is to adapt these specialized features to work with the new dataset, rather than overwrite the generic learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfxv_ifotQak"
   },
   "source": [
    "All you need to do is unfreeze the `base_model` and set the bottom layers to be un-trainable. Then, you should recompile the model (necessary for these changes to take effect), and resume training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4nzcagVitLQm",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#set the base model variable \"trainable\" to True\n",
    "print(base_model.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-4HgVAacRs5v",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's take a look to see how many layers are in the base model\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "\n",
    "# Fine-tune from this layer onwards\n",
    "fine_tune_at = ####THE LAST LAYER TO KEEP FROZEN\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "  layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Uk1dgsxT0IS"
   },
   "source": [
    "As you are training a much larger model and want to readapt the pretrained weights, it is important to use a lower learning rate at this stage. Otherwise, your model could overfit very quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NtUnaz0WUDva",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "              optimizer = tf.keras.optimizers.RMSprop(learning_rate=base_learning_rate/10),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WwBWy7J2kZvA",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0foWUN-yDLo_"
   },
   "source": [
    "Let's keep training from where we stopped before, so we can keep improving hopefully. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ECQLkAsFTlun",
    "tags": []
   },
   "outputs": [],
   "source": [
    "fine_tune_epochs = 10\n",
    "total_epochs =  initial_epochs + fine_tune_epochs\n",
    "\n",
    "history_fine = model.fit(train_dataset,\n",
    "                         epochs=total_epochs,\n",
    "                         initial_epoch=history.epoch[-1],\n",
    "                         validation_data=validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TfXEmsxQf6eP"
   },
   "source": [
    "Let's take a look at the learning curves of the training and validation accuracy/loss when fine-tuning the last few layers of the MobileNetV2 base model and training the classifier on top of it. Pay attention to not overfit!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PpA8PlpQKygw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc += history_fine.history['accuracy']\n",
    "val_acc += history_fine.history['val_accuracy']\n",
    "\n",
    "loss += history_fine.history['loss']\n",
    "val_loss += history_fine.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "chW103JUItdk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "          plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.ylim([0, 1.0])\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "         plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6cWgjgfrsn5"
   },
   "source": [
    "### Evaluation and prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSXH7PRMxOi5"
   },
   "source": [
    "Finaly you can verify the performance of the model on new data using test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2KyNhagHwfar",
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(test_dataset)\n",
    "print('Test accuracy :', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8UjS5ukZfOcR"
   },
   "source": [
    "And now you are all set to use this model to predict if the person is smiling or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RUNoQNgtfNgt",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve a batch of images from the test set\n",
    "image_batch, label_batch = test_dataset.as_numpy_iterator().next()\n",
    "predictions = model.predict_on_batch(image_batch).flatten()\n",
    "\n",
    "predictions = tf.where(predictions < 0.5, 0, 1)\n",
    "\n",
    "print('Predictions:\\n', predictions.numpy())\n",
    "print('Labels:\\n', label_batch)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(9):\n",
    "  ax = plt.subplot(3, 3, i + 1)\n",
    "  plt.imshow(image_batch[i].astype(\"uint8\"))\n",
    "  plt.title(class_names[predictions[i]])\n",
    "  plt.axis(\"off\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Test with your own selfie :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to upload a picture of you smiling or not and check whether the model can detect it correctly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TZTwG7nhm0C"
   },
   "source": [
    "### Wrap-up:\n",
    "\n",
    "* **Using a pre-trained model for feature extraction**:  When working with a small dataset, it is a common practice to take advantage of features learned by a model trained on a larger dataset in the same domain. This is done by instantiating the pre-trained model and adding a fully-connected classifier on top. The pre-trained model is \"frozen\" and only the weights of the classifier get updated during training.\n",
    "In this case, the convolutional base extracted all the features associated with each image and you just trained a classifier that determines the image class given that set of extracted features.\n",
    "\n",
    "* **Fine-tuning a pre-trained model**: To further improve performance, one might want to repurpose the top-level layers of the pre-trained models to the new dataset via fine-tuning.\n",
    "In this case, you tuned your weights such that your model learned high-level features specific to the dataset. Be aware that fine-tuning is not always the go-for-it solution!It depends\n",
    "\n",
    "Extra: see also [Transfer learning guide](https://www.tensorflow.org/guide/keras/transfer_learning).\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "transfer_learning.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
